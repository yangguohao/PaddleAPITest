torch.tensor(list[list[1.0, 2.0], list[3.0, 4.0]], dtype="float32", device="cuda")
torch.tensor(list[list[5.0, 6.0], list[7.0, 8.0]], device="cpu")
torch.Tensor.to(Tensor([2, 2], "float32"), device="cuda")
torch.Tensor.add(Tensor([2, 2], "float32"), Tensor([2, 2], "float32"))
torch.relu(Tensor([2, 2], "float32"))
torch.Tensor.add(Tensor([2, 2], "float32"), 10)
torch.nn.functional.softmax(Tensor([2, 2], "float32"), dim=1, _stacklevel=3, dtype=None)
torch.cat(tuple(Tensor([2, 2], "float32"), Tensor([2, 2], "float32")), dim=0)
torch.Tensor.argmax(Tensor([4, 2], "float32"), dim=0)
torch.tensor(list[-1.0, 0.0, 1.0, 2.0], requires_grad=True)
torch.Tensor.clamp(Tensor([4], "float32"), min=0)
torch.ones_like(Tensor([4], "float32"))
torch.Tensor.backward(Tensor([4], "float32"), gradient=Tensor([4], "float32"), retain_graph=None, create_graph=False, inputs=None)
custom_ops.custom_leaky_relu.default(Tensor([4], "float32"), 0.2)
torch.ones_like(Tensor([4], "float32"))
torch.autograd.grad(tuple(Tensor([4], "float32")), tuple(Tensor([4], "float32")), grad_outputs=Tensor([4], "float32"), retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False, is_grads_batched=False, materialize_grads=False)
